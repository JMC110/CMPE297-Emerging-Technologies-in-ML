{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.11"
    },
    "colab": {
      "name": "Extra_Credit_ReptileMAML.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJL9GMMVdCjJ"
      },
      "source": [
        "# Sine wave Regression Using Reptile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGwBOtnmdRQj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__ZyhV4UdCjK"
      },
      "source": [
        "\n",
        "In the last section, we saw how reptile works. Now, we will understand reptile better by coding them from scratch. Let us say we have a collection of tasks and the goal of each task is to regress the output of the sine wave given some input. So what do we mean by that?\n",
        "\n",
        "Let us say y = amplitude*sin(x+phase). The goal of our algorithm is to learn to regress the value of y given the x. The value of amplitude is chosen randomly between 0.1 and 5.0 and the value of phase is chosen randomly between 0 and $\\pi$. So for each of the tasks, we sample only 10 data points and train the network. i.e for each of the tasks, we sample only 10 (x,y) pairs. let us get to the code and see in detail.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6j6g4V1dCjK"
      },
      "source": [
        "First we import all the necessary libraries,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbOKwcCEdCjL"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FtTEHf6dCjO"
      },
      "source": [
        "## Generate Data Points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INJMc79adCjP"
      },
      "source": [
        "Now we define a function called sample_points for generating (x,y) pairs. It takes the parameter k as an input\n",
        "which implies number of (x,y) pairs we want to sample. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxL3GqIqdCjP"
      },
      "source": [
        "def sample_points(k):\n",
        "    \n",
        "    num_points = 100\n",
        "    \n",
        "    #amplitude\n",
        "    amplitude = np.random.uniform(low=0.1, high=5.0)\n",
        "    \n",
        "    #phase\n",
        "    phase = np.random.uniform(low=0, high=np.pi)\n",
        "\n",
        "    x = np.linspace(-5, 5, num_points)\n",
        "\n",
        "    #y = a*sin(x+b)\n",
        "    y = amplitude * np.sin(x + phase)\n",
        "    \n",
        "    #sample k data points\n",
        "    sample = np.random.choice(np.arange(num_points), size=k)\n",
        "    \n",
        "    return (x[sample], y[sample])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkMNCvvUdCjS"
      },
      "source": [
        "The above function returns output as follows,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "fr1kfV_hdCjS",
        "outputId": "520a128b-cc7b-4428-e117-94e9e44858c8"
      },
      "source": [
        "x, y = sample_points(5)\n",
        "print x\n",
        "print y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1.96969697 -2.17171717  3.18181818  1.06060606  2.67676768]\n",
            "[-0.97122995 -0.19398046 -1.1409783   0.07730279 -1.29343206]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTFcvD_YdCjV"
      },
      "source": [
        "## Two Layered Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvnf59rydCjV"
      },
      "source": [
        "Like MAML, reptile also compatible with any algorithms that can be trained with gradient descent. So we use a simple two layered neural network with 64 hidden units. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPV3nSRzdCjW"
      },
      "source": [
        "First, let's reset the tensorflow graph,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTze9lwWdCjX"
      },
      "source": [
        "tf.reset_default_graph()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fWM9xi8dCjZ"
      },
      "source": [
        "Initialize network parameters,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhmToUA4dCja"
      },
      "source": [
        "num_hidden = 64\n",
        "num_classes = 1\n",
        "num_feature = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DSGj3_ydCjc"
      },
      "source": [
        "Next, we define the placeholders for our input and output,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wooija2SdCjd"
      },
      "source": [
        "X = tf.placeholder(tf.float32, shape=[None, num_feature])\n",
        "Y = tf.placeholder(tf.float32, shape=[None, num_classes])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHb2evofdCjf"
      },
      "source": [
        "Randomly initialize our model parameters, "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfZXO_TrdCjg"
      },
      "source": [
        "w1 = tf.Variable(tf.random_uniform([num_feature, num_hidden]))\n",
        "b1 = tf.Variable(tf.random_uniform([num_hidden]))\n",
        "\n",
        "w2 = tf.Variable(tf.random_uniform([num_hidden, num_classes]))\n",
        "b2 = tf.Variable(tf.random_uniform([num_classes]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlrZF_CpdCji"
      },
      "source": [
        "Perform feedforward operation to predict the output Yhat"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wldiLVTdCji"
      },
      "source": [
        "#layer 1\n",
        "z1 = tf.matmul(X, w1) + b1\n",
        "a1 = tf.nn.tanh(z1)\n",
        "\n",
        "#output layer\n",
        "z2 = tf.matmul(a1, w2) + b2\n",
        "Yhat = tf.nn.tanh(z2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iPiXQ2RdCjk"
      },
      "source": [
        "We use mean squared error as our loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ii4l-DVFdCjl"
      },
      "source": [
        "loss_function = tf.reduce_mean(tf.square(Yhat - Y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj7NzRQ5dCjn"
      },
      "source": [
        "Minimize the loss using Adam Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kB4yBSzwdCjn"
      },
      "source": [
        "optimizer = tf.train.AdamOptimizer(1e-2).minimize(loss_function)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1hthgZ0dCjt"
      },
      "source": [
        "Initialize tensorflow variables,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp-VWFHedCjt"
      },
      "source": [
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaWgqtDLdCjv"
      },
      "source": [
        "## Reptile\n",
        "\n",
        "\n",
        "Now we will see how can we find the optimal parameters of our neural network with reptile. \n",
        "\n",
        "First we initialize necessary variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iITSJUKVdCjw"
      },
      "source": [
        "#number of epochs i.e training iterations\n",
        "num_epochs = 100\n",
        "\n",
        "\n",
        "#number of samples i.e number of shots\n",
        "num_samples = 50  \n",
        "\n",
        "#number of tasks\n",
        "num_tasks = 2\n",
        "\n",
        "#number of times we want to perform optimization\n",
        "num_iterations = 10\n",
        "\n",
        "\n",
        "#mini btach size\n",
        "mini_batch = 10  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "91xbDOWvdCjy",
        "outputId": "b961dabb-aa16-4c4d-d8e4-47d58aefe293"
      },
      "source": [
        "#start the tensorflow session\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    sess.run(init)\n",
        "    \n",
        "    for e in range(num_epochs):\n",
        "        \n",
        "        #for each task in batch of tasks\n",
        "        for task in range(num_tasks):\n",
        "\n",
        "            #get the initial parameters of the model\n",
        "            old_w1, old_b1, old_w2, old_b2 = sess.run([w1, b1, w2, b2,])\n",
        "\n",
        "            #sample x and y\n",
        "            x_sample, y_sample = sample_points(num_samples)\n",
        "\n",
        "\n",
        "            #for some k number of iterations perform optimization on the task\n",
        "            for k in range(num_iterations):\n",
        "\n",
        "                #get the minibatch x and y\n",
        "                for i in range(0, num_samples, mini_batch):\n",
        "\n",
        "                    #sample mini batch of examples \n",
        "                    x_minibatch = x_sample[i:i+mini_batch]\n",
        "                    y_minibatch = y_sample[i:i+mini_batch]\n",
        "\n",
        "\n",
        "                    train = sess.run(optimizer, feed_dict={X: x_minibatch.reshape(mini_batch,1), \n",
        "                                                           Y: y_minibatch.reshape(mini_batch,1)})\n",
        "\n",
        "            #get the updated model parameters after several iterations of optimization\n",
        "            new_w1, new_b1, new_w2, new_b2 = sess.run([w1, b1, w2, b2])\n",
        "\n",
        "            #Now we perform meta update\n",
        "\n",
        "            #i.e theta = theta + epsilon * (theta_star - theta)\n",
        "\n",
        "            epsilon = 0.1\n",
        "\n",
        "            updated_w1 = old_w1 + epsilon * (new_w1 - old_w1) \n",
        "            updated_b1 = old_b1 + epsilon * (new_b1 - old_b1) \n",
        "\n",
        "            updated_w2 = old_w2 + epsilon * (new_w2 - old_w2) \n",
        "            updated_b2 = old_b2 + epsilon * (new_b2 - old_b2) \n",
        "\n",
        "\n",
        "            #update the model parameter with new parameters\n",
        "            w1.load(updated_w1, sess)\n",
        "            b1.load(updated_b1, sess)\n",
        "\n",
        "            w2.load(updated_w2, sess)\n",
        "            b2.load(updated_b2, sess)\n",
        "\n",
        "        if e%10 == 0:\n",
        "            loss = sess.run(loss_function, feed_dict={X: x_sample.reshape(num_samples,1), Y: y_sample.reshape(num_samples,1)})\n",
        "\n",
        "            print \"Epoch {}: Loss {}\\n\".format(e,loss)             \n",
        "            print 'Updated Model Parameter Theta\\n'\n",
        "            print 'Sampling Next Batch of Tasks \\n'\n",
        "            print '---------------------------------\\n'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: Loss 4.162981987\n",
            "\n",
            "Updated Model Parameter Theta\n",
            "\n",
            "Sampling Next Batch of Tasks \n",
            "\n",
            "---------------------------------\n",
            "\n",
            "Epoch 10: Loss 1.52488529682\n",
            "\n",
            "Updated Model Parameter Theta\n",
            "\n",
            "Sampling Next Batch of Tasks \n",
            "\n",
            "---------------------------------\n",
            "\n",
            "Epoch 20: Loss 1.74668705463\n",
            "\n",
            "Updated Model Parameter Theta\n",
            "\n",
            "Sampling Next Batch of Tasks \n",
            "\n",
            "---------------------------------\n",
            "\n",
            "Epoch 30: Loss 0.955383658409\n",
            "\n",
            "Updated Model Parameter Theta\n",
            "\n",
            "Sampling Next Batch of Tasks \n",
            "\n",
            "---------------------------------\n",
            "\n",
            "Epoch 40: Loss 9.85152721405\n",
            "\n",
            "Updated Model Parameter Theta\n",
            "\n",
            "Sampling Next Batch of Tasks \n",
            "\n",
            "---------------------------------\n",
            "\n",
            "Epoch 50: Loss 3.62764883041\n",
            "\n",
            "Updated Model Parameter Theta\n",
            "\n",
            "Sampling Next Batch of Tasks \n",
            "\n",
            "---------------------------------\n",
            "\n",
            "Epoch 60: Loss 5.67536497116\n",
            "\n",
            "Updated Model Parameter Theta\n",
            "\n",
            "Sampling Next Batch of Tasks \n",
            "\n",
            "---------------------------------\n",
            "\n",
            "Epoch 70: Loss 1.39854609966\n",
            "\n",
            "Updated Model Parameter Theta\n",
            "\n",
            "Sampling Next Batch of Tasks \n",
            "\n",
            "---------------------------------\n",
            "\n",
            "Epoch 80: Loss 6.68237304688\n",
            "\n",
            "Updated Model Parameter Theta\n",
            "\n",
            "Sampling Next Batch of Tasks \n",
            "\n",
            "---------------------------------\n",
            "\n",
            "Epoch 90: Loss 7.83017683029\n",
            "\n",
            "Updated Model Parameter Theta\n",
            "\n",
            "Sampling Next Batch of Tasks \n",
            "\n",
            "---------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}